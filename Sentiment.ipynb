{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries \n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the words into the their base form by using the WordNetLemmatizer from nltk\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "# removing the useless words which give us no inference, the words are present in th stopwords.txt\n",
    "stopwords = set(w.rstrip() for w in open('stopwords.txt'))\n",
    "\n",
    "#using BeautifulSoup to parse through the Xml data and creating the positive reviews section\n",
    "postive_reviews = BeautifulSoup(open('electronics/positive.review').read())\n",
    "# only using the review text from the postive_reviews file\n",
    "postive_reviews = postive_reviews.findAll('review_text')\n",
    "\n",
    "# using BeautifulSoup to parse through the XML data and creating the negative_reviews\n",
    "negative_reviews = BeautifulSoup(open('electronics/negative.review').read())\n",
    "# only using the review text from the negative_reviews file \n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to ensure that there are equal no. of positive and negative reviews \n",
    "# so we need to shuffle the postive reviews and cut the extra \n",
    "\n",
    "# shuffling the positive reviews \n",
    "np.random.shuffle(postive_reviews)\n",
    "# ensuring that the size of postive_reviews and negative_reviews is the same \n",
    "postive_reviews = postive_reviews[: len(negative_reviews)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out the number of words and what all words we have in our document \n",
    "\n",
    "# building our tokenizer \n",
    "def my_tokenizer(s):\n",
    "    # firstly lower case the entire string so that capitlized and non capitalized words are not different\n",
    "    s = s.lower()\n",
    "    # seperating the words in the string and putting them in an array\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    # removing all the words which are shorter than 2 letters because they obviously don't make any meaning\n",
    "    tokens = [t for t in tokens if len(t)>2]\n",
    "    # lemmatizing the words\n",
    "    tokens = [word_lemmatizer.lemmatize(t) for t in tokens]\n",
    "    # removing the stopwords from our dictionary \n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    # returning the final array\n",
    "    return tokens \n",
    "    \n",
    "    \n",
    "word_index_map = {} # tells us about what all words we have \n",
    "current_index = 0 # tells us about the number of words we have \n",
    "\n",
    "# storing positive and negative tokenized strings in arrays \n",
    "postive_tokenized = []\n",
    "negative_tokenized = []\n",
    "\n",
    "# now we need to tokenize the string\n",
    "# that is convert words in a string to words in an array \n",
    "\n",
    "# tokenizing the positive reviews and adding the words into the dictionary \n",
    "for review in postive_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    postive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "            \n",
    "# tokenizing the negative reviews and adding the words into the dictionary \n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to convert the tokens into vectors\n",
    "# so that we can run we can use machine learning over it \n",
    "\n",
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # last element is for the label\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum() # normalize it before setting label\n",
    "    x[-1] = label\n",
    "    return x\n",
    "\n",
    "N = len(postive_tokenized) + len(negative_tokenized)\n",
    "# (N x D+1 matrix - keeping them together for now so we can shuffle more easily later\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "for tokens in postive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "# shuffle the data and create train/test splits\n",
    "# try it multiple times!\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]\n",
    "\n",
    "# last 100 rows will be test\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achintya/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# making the machine learning model \n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Classification rate:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding all the numbers which have been missclassified \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
